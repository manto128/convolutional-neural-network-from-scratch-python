{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "(train_images, train_labels), (test_images, test_labels)= datasets.mnist.load_data()\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 32, 32])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad input 28x28 images with zeros to 32x32 images and scaled 8-bit pixel values to values between 0-1\n",
    "train_images = tf.pad(train_images, [[0, 0], [2,2], [2,2]])/255\n",
    "test_images = tf.pad(test_images, [[0, 0], [2,2], [2,2]])/255\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 32, 32, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = tf.expand_dims(train_images, axis=3, name=None)\n",
    "test_images = tf.expand_dims(test_images, axis=3, name=None)\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = train_images[-2000:,:,:,:] \n",
    "val_labels = train_labels[-2000:] \n",
    "train_images = train_images[:-2000,:,:,:] \n",
    "train_labels = train_labels[:-2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 14, 14, 6)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 5, 5, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 1, 1, 120)         48120     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61706 (241.04 KB)\n",
      "Trainable params: 61706 (241.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(6, 5, activation='relu6', input_shape=train_images.shape[1:]))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "# model.add(layers.Activation('sigmoid'))\n",
    "model.add(layers.Conv2D(16, 5, activation='relu6'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "# model.add(layers.Activation('sigmoid'))\n",
    "model.add(layers.Conv2D(120, 5, activation='relu6'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(84, activation='relu6'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "907/907 [==============================] - 16s 17ms/step - loss: 0.2291 - accuracy: 0.9312 - val_loss: 0.0737 - val_accuracy: 0.9820\n",
      "Epoch 2/10\n",
      "907/907 [==============================] - 17s 18ms/step - loss: 0.0725 - accuracy: 0.9773 - val_loss: 0.0651 - val_accuracy: 0.9865\n",
      "Epoch 3/10\n",
      "907/907 [==============================] - 15s 16ms/step - loss: 0.0519 - accuracy: 0.9832 - val_loss: 0.0563 - val_accuracy: 0.9870\n",
      "Epoch 4/10\n",
      "907/907 [==============================] - 15s 17ms/step - loss: 0.0424 - accuracy: 0.9862 - val_loss: 0.0518 - val_accuracy: 0.9865\n",
      "Epoch 5/10\n",
      "907/907 [==============================] - 19s 21ms/step - loss: 0.0343 - accuracy: 0.9886 - val_loss: 0.0530 - val_accuracy: 0.9870\n",
      "Epoch 6/10\n",
      "907/907 [==============================] - 24s 26ms/step - loss: 0.0294 - accuracy: 0.9905 - val_loss: 0.0502 - val_accuracy: 0.9895\n",
      "Epoch 7/10\n",
      "907/907 [==============================] - 35s 39ms/step - loss: 0.0249 - accuracy: 0.9916 - val_loss: 0.0436 - val_accuracy: 0.9895\n",
      "Epoch 8/10\n",
      "907/907 [==============================] - 39s 43ms/step - loss: 0.0206 - accuracy: 0.9932 - val_loss: 0.0460 - val_accuracy: 0.9915\n",
      "Epoch 9/10\n",
      "907/907 [==============================] - 35s 38ms/step - loss: 0.0182 - accuracy: 0.9934 - val_loss: 0.0405 - val_accuracy: 0.9915\n",
      "Epoch 10/10\n",
      "907/907 [==============================] - 34s 37ms/step - loss: 0.0167 - accuracy: 0.9946 - val_loss: 0.0459 - val_accuracy: 0.9895\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "history = model.fit(train_images, train_labels, batch_size=64, epochs=10, validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        # Model has only one input so each data point has one element.\n",
    "        yield [input_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/xs/nn2f1m4d4vg3mp72k2gv6c8h0000gn/T/tmpm1p672ss/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/xs/nn2f1m4d4vg3mp72k2gv6c8h0000gn/T/tmpm1p672ss/assets\n",
      "/Users/abhishek/Desktop/PhD/Research/ML_and_AI/Models/.venv/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:947: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-11-09 23:11:43.171724: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-09 23:11:43.171744: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-09 23:11:43.172160: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/xs/nn2f1m4d4vg3mp72k2gv6c8h0000gn/T/tmpm1p672ss\n",
      "2023-11-09 23:11:43.174788: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-09 23:11:43.174807: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/xs/nn2f1m4d4vg3mp72k2gv6c8h0000gn/T/tmpm1p672ss\n",
      "2023-11-09 23:11:43.178866: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "2023-11-09 23:11:43.180905: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-09 23:11:43.260865: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/xs/nn2f1m4d4vg3mp72k2gv6c8h0000gn/T/tmpm1p672ss\n",
      "2023-11-09 23:11:43.285501: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 113345 microseconds.\n",
      "2023-11-09 23:11:43.311065: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.int8'>\n",
      "output:  <class 'numpy.int8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70328"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"../saved_models\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized model:\n",
    "tf_model_file = tflite_models_dir/\"lenet5.keras\"\n",
    "model.save(tf_model_file)\n",
    "\n",
    "# Save the quantized model:\n",
    "tflite_model_quant_file = tflite_models_dir/\"lenet5_int8.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global test_images\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = test_images[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to int8\n",
    "    if input_details['dtype'] == np.int8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global test_images\n",
    "  global test_labels\n",
    "\n",
    "  test_image_indices = range(test_images.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int8 model accuracy is 98.9000% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_quant_file, model_type=\"Int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path='../saved_models/lenet5_int8.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "\n",
    "tensor_details = tflite_interpreter.get_tensor_details()\n",
    "num_fc_layers = 2\n",
    "num_conv2d_layers = 3\n",
    "\n",
    "obj = []\n",
    "cache = []\n",
    "\n",
    "for dict in tensor_details:\n",
    "    # print(dict)\n",
    "    i = dict['index']\n",
    "    name = dict['name']\n",
    "    shape = dict['shape']\n",
    "    if ';' not in name:\n",
    "        if 'BiasAdd' in name:\n",
    "            bias = tflite_interpreter.tensor(i)()\n",
    "            # print(i, name, shape)\n",
    "            # print(bias)\n",
    "        if 'MatMul' in name:\n",
    "            weights = tflite_interpreter.tensor(i)()\n",
    "            reshaped_weights = np.transpose(weights)\n",
    "            # print(i, name, reshaped_weights.shape)\n",
    "            # print(reshaped_weights)\n",
    "            cache = {'fc' + str(num_fc_layers) + '.weights': reshaped_weights, 'fc' + str(num_fc_layers) + '.bias': bias}\n",
    "            obj.append(cache)\n",
    "            num_fc_layers -= 1\n",
    "        if name.split('/')[-1] == 'Conv2D':\n",
    "            weights = tflite_interpreter.tensor(i)()\n",
    "            # print(i, name, shape)\n",
    "            reshaped_weights = np.zeros(dtype=np.int8, shape=(weights.shape[0], weights.shape[3], weights.shape[1], weights.shape[2]))\n",
    "            for l in range(weights.shape[0]):\n",
    "                for k in range(weights.shape[1]):\n",
    "                    for j in range(weights.shape[2]):\n",
    "                        for i in range(weights.shape[3]):\n",
    "                            reshaped_weights[l][i][k][j] = weights[l][k][j][i]\n",
    "            # print(i, name, reshaped_weights.shape)\n",
    "            # print(reshaped_weights)\n",
    "            cache = {'conv' + str(num_conv2d_layers) + '.weights': reshaped_weights, 'conv' + str(num_conv2d_layers) + '.bias': bias}\n",
    "            obj.append(cache)\n",
    "            num_conv2d_layers -= 1\n",
    "\n",
    "with open('./params.pkl', 'wb') as handle:\n",
    "    pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1, 5, 5) (6,)\n",
      "(16, 6, 5, 5) (16,)\n",
      "(120, 16, 5, 5) (120,)\n",
      "(120, 84) (84,)\n",
      "(84, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "with open('./params.pkl', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "    print(b[4]['conv1.weights'].shape, b[4]['conv1.bias'].shape)\n",
    "    print(b[3]['conv2.weights'].shape, b[3]['conv2.bias'].shape)\n",
    "    print(b[2]['conv3.weights'].shape, b[2]['conv3.bias'].shape)\n",
    "    print(b[1]['fc1.weights'].shape, b[1]['fc1.bias'].shape)\n",
    "    print(b[0]['fc2.weights'].shape, b[0]['fc2.bias'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
